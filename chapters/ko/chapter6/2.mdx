# 기존 토크나이저로부터 새 토크나이저 학습하기[[training-a-new-tokenizer-from-an-old-one]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb"},
]} />

원하는 언어로 사용 가능한 언어 모델이 없거나 귀하의 말뭉치가 귀하의 언어 모델이 훈련된 것과 매우 다른 경우, 아마도 귀하는 귀하의 데이터에 적합한 토크나이저를 사용하여 모델을 처음부터 다시 훈련하고 싶을 것입니다. 이는 귀하의 데이터셋에서 새로운 토크나이저를 훈련해야 함을 의미합니다. 그러나 이게 정확히 무엇을 의미하는 걸까요? [Chapter 2](/course/chapter2)에서 토크나이저를 처음 살펴볼 때 대부분의 Transformer 모델이 _서브워드 토큰화 알고리즘_을 사용한다는 것을 보았습니다. 관심 있는 서브워드를 식별하고 해당 말뭉치에서 가장 빈번하게 발생하는 서브워드를 확인하기 위해 토크나이저는 말뭉치의 모든 텍스트를 신중하게 살펴봐야 합니다. 이러한 과정을 *훈련*이라고 합니다. 이 훈련을 지배하는 정확한 규칙은 사용된 토크나이저 유형에 따라 다르며, 이 장의 후반부에서 세 가지 주요 알고리즘을 자세히 다룰 것입니다.

<Youtube id="DJimQynXZsQ"/>

<Tip warning={true}>

⚠️ 토크나이저를 훈련하는 것은 모델을 훈련하는 것과 같지 않습니다! 모델 훈련은 확률적 경사 하강법을 사용하여 각 배치마다 손실을 조금씩 줄입니다. 이는 본질적으로 무작위성을 가지며 (동일한 훈련을 두 번 실행할 때 동일한 결과를 얻으려면 일부 시드를 설정해야 함) 토크나이저 훈련은 주어진 말뭉치에서 어떤 서브워드를 선택해야 하는지 식별하기 위한 통계적 과정입니다. 사용된 정확한 규칙은 토큰화 알고리즘에 따라 달라집니다. 이는 결정적이며, 동일한 알고리즘과 말뭉치에서 훈련할 때 항상 동일한 결과를 얻습니다.

</Tip>

## 말뭉치 조합[[말뭉치 조합]]

🤗 Transformers에는 기존 특성과 유사한 새로운 토크나이저를 훈련하는 매우 간단한 API가 있습니다: `AutoTokenizer.train_new_from_iterator()` 이를 실제로 보려면, 예를 들어 GPT-2를 처음부터 영어 이외의 다른 언어로 훈련하고 싶다고 가정해 보겠습니다. 첫 번째 작업은 해당 언어의 훈련 말뭉치에서 많은 데이터를 수집하는 것입니다. 모두가 이해할 수 있는 예제를 제공하기 위해, 여기서는 러시아어나 중국어와 같은 언어를 사용하지 않고 특수화된 영어 언어인 Python 코드를 사용할 것입니다.

[🤗 Datasets](https://github.com/huggingface/datasets) 라이브러리를 사용하여 Python 소스 코드의 말뭉치를 조합할 수 있습니다. 우리는 [CodeSearchNet](https://huggingface.co/datasets/code_search_net) 데이터셋을 다운로드하고 캐시에 저장하기 위해 일반적인 `load_dataset()` 함수를 사용할 것입니다. 이 데이터셋은 [CodeSearchNet 챌린지](https://wandb.ai/github/CodeSearchNet/benchmark)를 위해 만들어졌으며 GitHub의 여러 프로그래밍 언어에 대한 공개 라이브러리의 수백만 개 함수를 포함하고 있습니다. 여기서는 이 데이터셋의 Python 부분을 로드할 것입니다:

```py
from datasets import load_dataset

# 이 작업은 로드하는 데 몇 분 정도 걸릴 수 있으므로 기다리는 동안 커피나 차를 마시세요!
raw_datasets = load_dataset("code_search_net", "python")
```

훈련 분할을 살펴보면 액세스할 수 있는 열이 어떤 것인지 알 수 있습니다:

```py
raw_datasets["train"]
```

```python out
Dataset({
    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 
      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 
      'func_code_url'
    ],
    num_rows: 412178
})
```

데이터셋은 문서 문자열을 코드에서 분리하고 둘 다 토큰화를 제안합니다. 여기서는 토크나이저를 훈련하기 위해 `whole_func_string` 열만 사용할 것입니다. `train` 분할에 인덱싱하여 이러한 함수 중 하나의 예제를 살펴볼 수 있습니다:

```py
print(raw_datasets["train"][123456]["whole_func_string"])
```

아래 내용이 출력되어야 합니다:

```out
def handle_simple_responses(
      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):
    """장치로부터 일반 응답을 수락합니다.

    Args:
      timeout_ms: 각 응답을 기다리는 시간 초과(ms)입니다.
      info_cb: 부트로더에서 보낸 텍스트에 대한 선택적 콜백입니다.

    Returns:
      OKAY 패킷의 메시지.
    """
    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)
```

첫 번째로 해야 할 일은 데이터셋을 텍스트 리스트의 _반복자_로 변환하는 것입니다. 예를 들어, 텍스트 리스트의 리스트로 변환하는 것입니다. 텍스트 리스트를 사용하면 토크나이저가 빠르게 진행될 수 있습니다 (텍스트를 하나씩 처리하는 대신 텍스트 배치로 훈련) 및 한 번에 모든 것을 메모리에 로드하지 않도록 하려면 반복자여야 합니다. 말뭉치가 거대한 경우, 🤗 Datasets가 모든 것을 RAM에 로드하지 않고 데이터셋의 요소를 디스크에 저장하기 때문에 이를 활용하고 싶을 것입니다.

다음은 각각 1,000개의 텍스트로 구성된 텍스트 리스트의 리스트를 만들지만 모든 것을 메모리에 로드합니다:

```py
# 데이터셋이 작은 경우를 제외하고는 아래 줄을 주석 처리하지 마세요!
# training_corpus = [raw_datasets["train"][i: i + 1000]["whole_func_string"] for i in range(0, len(raw_datasets["train"]), 1000)]
```

파이썬 생성기를 사용하면 파이썬이 실제로 필요할 때까지 아무 것도 메모리에 로드하지 않을 수 있습니다. 이러한 생성기를 생성하려면 대괄호를 괄호로 바꾸기만 하면 됩니다:

```py
training_corpus = (
    raw_datasets["train"][i : i + 1000]["whole_func_string"]
    for i in range(0, len(raw_datasets["train"]), 1000)
)
```

이 코드 라인은 데이터셋의 요소를 가져오지 않고, 단지 Python `for` 루프에서 사용할 수 있는 객체를 생성합니다. 텍스트는 필요할 때만 로드되며 (즉, 필요한 단계의 `for` 루프에 있을 때) 한 번에 1,000개의 텍스트만 로드됩니다. 이렇게하면 거대한 데이터셋을 처리하더라도 메모리를 고갈시키지 않습니다.

생성기 객체의 문제점은 한 번만 사용할 수 있다는 것입니다. 따라서 첫 10개의 숫자 리스트를 두 번씩 제공하는 대신에 한 번 제공한 후에는 빈 리스트를 제공합니다:

```py
gen = (i for i in range(10))
print(list(gen))
print(list(gen))
```

we get them once and then an empty list:

```python out
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[]
```

그래서 우리는 대신 생성기를 반환하는 함수를 정의합니다:

```py
def get_training_corpus():
    return (
        raw_datasets["train"][i : i + 1000]["whole_func_string"]
        for i in range(0, len(raw_datasets["train"]), 1000)
    )


training_corpus = get_training_corpus()
```

또한 `yield` 문을 사용하여 `for` 루프 내에서 생성기를 정의할 수 있습니다:

```py
def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["whole_func_string"]
```

이전과 정확히 동일한 생성기를 생성하지만 리스트 내포보다 더 복잡한 로직을 사용할 수 있습니다.

## 새로운 토크나이저 훈련하기[[training-a-new-tokenizer]]

이제 우리가 텍스트 배치의 반복자 형태로 말뭉치를 가지고 있으므로 새로운 토크나이저를 훈련할 준비가 되었습니다. 이를 위해서는 먼저 모델과 짝을 이룰 토크나이저를 로드해야 합니다 (여기서는 GPT-2):

```py
from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

새로운 토크나이저를 훈련할 것이지만, 처음부터 시작하는 것을 피하려면 이것을 하는 것이 좋습니다. 이렇게 하면 토크나이제이션 알고리즘이나 사용할 특수 토큰에 대해 아무것도 지정할 필요가 없으며, 새로운 토크나이저는 GPT-2와 완전히 동일할 것이며 유일하게 변경되는 것은 말뭉치에서의 훈련에 의해 결정되는 어휘입니다.

먼저 이 토크나이저가 예제 함수를 어떻게 처리하는지 살펴보겠습니다:

```py
example = '''def add_numbers(a, b):
    """Add the two numbers `a` and `b`."""
    return a + b'''

tokens = old_tokenizer.tokenize(example)
tokens
```

```python out
['def', 'Ġadd', '_', 'n', 'umbers', '(', 'a', ',', 'Ġb', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ"""', 'Add', 'Ġthe', 'Ġtwo',
 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`', '."', '""', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']
```

이 토크나이저에는 `Ġ`와 `Ċ`와 같은 몇 가지 특별한 기호가 있으며, 각각 공백과 줄바꿈을 나타냅니다. 보면, 이것은 너무 효율적이지 않습니다: 토크나이저는 각 공백에 대해 개별 토큰을 반환하는데, 코드에서 네 개 또는 여덟 개의 공백 세트가 매우 흔할 것이므로 이것들을 그룹화 할 수 있었을 것입니다. 또한 함수 이름을 약간 이상하게 분리했는데, `_` 문자가 포함된 단어를 볼 때 익숙하지 않기 때문입니다.

이제 새로운 토크나이저를 훈련하고 이러한 문제들을 해결할 수 있는지 살펴보겠습니다. 이를 위해 `train_new_from_iterator()` 메소드를 사용하겠습니다:

```py
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
```

이 명령은 데이터셋이 매우 큰 경우 시간이 오래 걸릴 수 있지만, 이 1.6GB의 텍스트 데이터셋에 대해서는 엄청나게 빠릅니다 (AMD Ryzen 9 3900X CPU의 12 코어에서 1분 16초).

`AutoTokenizer.train_new_from_iterator()`은 사용 중인 토크나이저가 "빠른" 토크나이저인 경우에만 작동합니다. 다음 섹션에서 보겠지만, 🤗 Transformers 라이브러리에는 두 종류의 토크나이저가 있습니다: 순수 파이썬으로 작성된 것과 (빠른 것) 러스트 프로그래밍 언어로 작성된 🤗 Tokenizers 라이브러리를 지원하는 것입니다. 파이썬은 주로 데이터 과학 및 딥 러닝 응용 프로그램에 사용되는 언어이지만, 빠르게 처리하기 위해 병렬화해야 하는 경우 다른 언어로 작성해야 합니다. 예를 들어, 모델 계산의 핵심인 행렬 곱셈은 GPU에 대한 최적화된 C 라이브러리 인 CUDA로 작성됩니다.

순수 파이썬에서 완전히 새로운 토크나이저를 훈련하는 것은 극도로 느릴 것입니다. 이것이 🤗 Tokenizers 라이브러리를 개발한 이유입니다. CUDA 언어를 배우지 않고도 GPU에서 일괄 입력을 실행할 수 있는 것과 마찬가지로, 빠른 토크나이저를 사용하려면 Rust를 배울 필요가 없습니다. 🤗 Tokenizers 라이브러리는 내부적으로 일부 러스트 코드를 호출하는 많은 메소드에 대한 Python 바인딩을 제공합니다. 예를 들어, 새로운 토크나이저의 훈련이나 [Chapter 3](/course/chapter3)에서 본 것처럼 입력의 일괄 처리의 토크나이제이션을 병렬화하기 위해 사용됩니다.

대부분의 Transformer 모델은 빠른 토크나이저가 있습니다 (확인할 수 있는 예외가 있습니다. [여기](https://huggingface.co/transformers/#supported-frameworks)에서 확인할 수 있습니다.) 그리고 사용 가능한 경우 `AutoTokenizer` API는 항상 빠른 토크나이저를 선택합니다. 다음 섹션에서는 빠른 토크나이저가 가지고 있는 일부 다른 특수 기능을 살펴보겠습니다. 이러한 특수 기능은 토큰 분류 및 질문 응답과 같은 작업에 매우 유용할 것입니다. 그러나 그 전에 우리의 새로운 토크나이저를 이전 예제에서 적용해 보겠습니다:

```py
tokens = tokenizer.tokenize(example)
tokens
```

```python out
['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', 'Ġ"""', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`',
 'a', '`', 'Ġand', 'Ġ`', 'b', '`."""', 'ĊĠĠĠ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']
```

여기에서 다시 공백과 줄바꿈을 나타내는 특별한 기호 `Ġ`와 `Ċ`를 볼 수 있지만, 우리는 Python 함수 말뭉치에 특화된 일부 토큰도 배울 수 있습니다. 예를 들어, 들여쓰기를 나타내는 `ĊĠĠĠ` 토큰과 독스트링을 시작하는 세 개의 따옴표를 나타내는 `Ġ"""` 토큰이 있습니다. 또한 토크나이저는 `_`를 분리하고 함수 이름을 올바르게 분리했습니다. 이는 상당히 간결한 표현입니다. 비교적, 동일한 예제에 일반 영어 토크나이저를 사용하면 더 긴 문장이 생성됩니다:

```py
print(len(tokens))
print(len(old_tokenizer.tokenize(example)))
```

```python out
27
36
```

다른 예제를 살펴보겠습니다:

```python
example = """class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    """
tokenizer.tokenize(example)
```

```python out
['class', 'ĠLinear', 'Layer', '():', 'ĊĠĠĠ', 'Ġdef', 'Ġ__', 'init', '__(', 'self', ',', 'Ġinput', '_', 'size', ',',
 'Ġoutput', '_', 'size', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'weight', 'Ġ=', 'Ġtorch', '.', 'randn', '(', 'input', '_',
 'size', ',', 'Ġoutput', '_', 'size', ')', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'bias', 'Ġ=', 'Ġtorch', '.', 'zeros', '(',
 'output', '_', 'size', ')', 'ĊĊĠĠĠ', 'Ġdef', 'Ġ__', 'call', '__(', 'self', ',', 'Ġx', '):', 'ĊĠĠĠĠĠĠĠ',
 'Ġreturn', 'Ġx', 'Ġ@', 'Ġself', '.', 'weights', 'Ġ+', 'Ġself', '.', 'bias', 'ĊĠĠĠĠ']
```

여기에서 들여쓰기에 해당하는 토큰에 추가로 두 번의 들여쓰기를 나타내는 토큰인 `ĊĠĠĠĠĠĠĠ`를 볼 수 있습니다. `class`, `init`, `call`, `self`, `return`과 같은 특수 Python 단어는 각각 하나의 토큰으로 토크나이즈되며, `_`와 `.`을 분리하는 것뿐만 아니라 camel-cased 이름도 올바르게 분리합니다: `LinearLayer`는 `["ĠLinear", "Layer"]`로 토크나이즈됩니다.

## 토크나이저 저장하기[[saving-the-tokenizer]]

나중에 사용할 수 있도록 새로운 토크나이저를 저장해야 합니다. 모델과 마찬가지로, 이 작업은 `save_pretrained()` 메소드로 수행됩니다:

```py
tokenizer.save_pretrained("code-search-net-tokenizer")
```

이 명령은 *code-search-net-tokenizer*라는 새 폴더를 생성하고, 토크나이저가 다시 로드되기 위해 필요한 모든 파일이 포함됩니다. 이 토크나이저를 동료나 친구들과 공유하려면 계정에 로그인하여 허브에 업로드할 수 있습니다. 노트북에서 작업 중인 경우 이 작업을 돕는 편리한 기능이 있습니다:

```python
from huggingface_hub import notebook_login

notebook_login()
```

이것은 Hugging Face 로그인 자격 증명을 입력할 수 있는 위젯을 표시합니다. 노트북에서 작업 중이 아닌 경우 터미널에서 다음 줄을 입력하십시오:

```bash
huggingface-cli login
```

로그인한 후에 다음 명령을 실행하여 토크나이저를 푸시할 수 있습니다:

```py
tokenizer.push_to_hub("code-search-net-tokenizer")
```

이 명령은 토크나이저 파일을 포함한 `code-search-net-tokenizer`라는 이름의 새 리포지토리를 네임스페이스에 생성합니다. 그런 다음 `from_pretrained()` 메소드를 사용하여 어디서든 토크나이저를 로드할 수 있습니다:

```py
# Replace "huggingface-course" below with your actual namespace to use your own tokenizer
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")
```

이제 모델을 처음부터 훈련하고 현재 작업에 대해 모델을 세밀하게 조정할 준비가 되었습니다! [Chapter 7](/course/chapter7)에서 이를 수행할 것입니다. 그러나 먼저 이 장의 나머지 부분에서 빠른 토크나이저를 더 자세히 살펴보고 `train_new_from_iterator()` 메소드를 호출할 때 실제로 무엇이 발생하는지 자세히 살펴보겠습니다.
